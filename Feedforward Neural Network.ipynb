{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(history):\n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.plot(history.history['loss'], linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], linewidth=2)\n",
    "    plt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.title('Loss Curve', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_curve(history):\n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.plot(history.history['accuracy'], linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], linewidth=2)\n",
    "    plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.title('Accuracy Curve', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a model for recognizing handwritten digits, 0 to 9. The MNIST dataset is a common dataset used to showcase machine learning and deep learning methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have 60k samples in the training dataset and 10k in the test dataset. Each image is the size of 28x28 pixels.\n",
    "\n",
    "Let's look at what some of these images are like. Change the index below to look at different training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "plt.figure(figsize=[6, 6])\n",
    "plt.imshow(X_train[idx,:,:])\n",
    "plt.title(y_train[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we feed the data into the neural network is by \"flattening\" the images so that instead of the matrix format they are in the array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = np.prod(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to transform our images so that instead of a 28x28 matrix we have a 784-long array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], n_features)\n",
    "X_test = X_test.reshape(X_test.shape[0], n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it did what it should have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for our pixels are 0-255. We would like to normalize them so they are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to transform our labels. In the MNIST dataset, labels are integers 0-9. Neural network we will build will have as many neurons in the output layers as there are labels, each neuron representing one class. We now want to transform our labels using so-called `one-hot encoding`. In this process, each integer label is converted into an array with all elements equal to zero, except the one corresponding to the label's class. For example, 0 will be represented as \\[0 0 0 0 0 0 0 0 0 0\\], 1 as \\[0 1 0 0 0 0 0 0 0 0], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to define models in Keras. In `Sequential` we stack up the layers one by one. It is a pretty intiutive method and the one we will use now. `Functional API` models are used for more complex use cases such as models with shared layers or multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding the layers, we need to specify the number of units, activation function, and for the first layer the shape of the input, i.e. the number of the features. For the later layers the network figures out the input shape on its own, i.e. the input shape parameter for the next layer is equal to the number of nodes of the previous layer.\n",
    "\n",
    "We will use ReLU activation function for all hidden layers, and softmax for the last layer. Softmax can be thought of as logistic regression for the case of multiple classes. It outputs probabilities that the sample belongs to one of the (in our case 10) classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(32, activation='relu', input_shape=(n_features,)))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need to configure the network by specifying the optimizer we will use, the loss function and the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.summary()` to find out how many parameters our model has to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy  = model.evaluate(X_test, y_test, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_curve(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While keeping everything else fixed, run the same model for 10, 20, 30 epochs.\n",
    "* What is the accuracy of different neural networks?\n",
    "* How do the loss curves look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While keeping everything else fixed, run the same model for the hidden layer with 64, 128, 256, and 512 units.\n",
    "* What is the accuracy of different neural networks?\n",
    "* How do the loss curves look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While keeping everything else fixed, run the same model with 2, 3, and 4 hidden layers.\n",
    "* What is the accuracy of different neural networks?\n",
    "* How do the loss curves look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While keeping everything else fixed, try other optimization methods (RMSprop, Adam).\n",
    "* What is the accuracy of different neural networks?\n",
    "* How do the loss curves look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel like an extra task, add regularization to the model and see how it changes the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
